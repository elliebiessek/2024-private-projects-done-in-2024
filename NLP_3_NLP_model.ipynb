{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info alertinfo\" style=\"margin-top: 0px\">\n",
    "<h1> Natural Language Processing with Disaster Tweets </h1>\n",
    "part 3 - machine learning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\" style=\"margin-top: 0px\">\n",
    "<h1> Imports </h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# others\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. read data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv('test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\" style=\"margin-top: 0px\">\n",
    "<h1> Data cleaning </h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-clean and corpus set up\n",
    "first cleaning function that will be updated later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions 1\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_emails(text):\n",
    "    text = re.sub(r'\\S+@\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'(^|\\s)@\\w+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_foreign_characters(text):\n",
    "    text = re.sub(r'([^\\x00-\\x7F])+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_short_words(text):\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stopwords])\n",
    "\n",
    "def remove_symbols_and_numbers(text):\n",
    "    text = ''.join(' ' if not c.isalpha() else c for c in text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def clean_phase_1(text):\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_emails(text)\n",
    "    text = remove_foreign_characters(text)\n",
    "    text = remove_symbols_and_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_short_words(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedy', 'derail', 'typhoon', 'terrorism', 'arson']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words = set(words.words())\n",
    "\n",
    "# find 'important words' - 'keywords' in our training set\n",
    "data = df.copy()\n",
    "feature_name = 'keyword'\n",
    "data[feature_name] = data[feature_name].apply(clean_phase_1)\n",
    "set_values = set(data[feature_name].values)\n",
    "set_values.remove('')\n",
    "important_words = set()\n",
    "for value in set_values:\n",
    "    words = value.split()\n",
    "    important_words.update(words)\n",
    "    \n",
    "examples = list(important_words)[:5]\n",
    "examples   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lynnfield', 'babby', 'cocoa', 'tensift', 'topeka']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find 'unimportant words' - 'locations' in our training set\n",
    "feature_name = 'location'\n",
    "data[feature_name] = data[feature_name].apply(clean_phase_1)\n",
    "set_values = ' '.join(data[feature_name].dropna())\n",
    "set_values = set_values.split()\n",
    "set_values = set([word for word in set_values if len(word) > 2])\n",
    "unimportant_words = set()\n",
    "for value in set_values:\n",
    "    words = value.split()\n",
    "    unimportant_words.update(words)\n",
    "words_to_keep = unimportant_words.intersection(important_words)\n",
    "unimportant_words = unimportant_words - words_to_keep\n",
    "\n",
    "examples = list(unimportant_words)[:5]\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating corpus\n",
    "english_words.update(important_words)\n",
    "english_words = english_words - unimportant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data words\n",
    "feature_name = 'text'\n",
    "data[feature_name] = data[feature_name].apply(clean_phase_1)\n",
    "all_text = ' '.join(data['text'].dropna())\n",
    "words = all_text.split()\n",
    "word_counts = Counter(words)\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index', columns=['count']).reset_index()\n",
    "word_counts_df.columns = ['word', 'count']\n",
    "word_counts_df = word_counts_df.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "data_words_df = word_counts_df[word_counts_df['count'] >= 5]\n",
    "train_data_words = set(data_words_df['word'].values)\n",
    "\n",
    "train_words_common = train_data_words.intersection(english_words)\n",
    "train_words_uncommon = train_data_words - train_words_common\n",
    "\n",
    "# updating corpus\n",
    "english_words.update(train_data_words) # all words on the first phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. prepare for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>reason earthquak may allah forgiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                   Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                              Forest fire near La Ronge Sask. Canada   \n",
       "2   5                   All residents asked to 'shelter in place' are ...   \n",
       "3   6                   13,000 people receive #wildfires evacuation or...   \n",
       "4   7                   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                     processed text  \n",
       "0       1                  reason earthquak may allah forgiv  \n",
       "1       1                            forest fire near canada  \n",
       "2       1  resid ask shelter place notifi offic evacu she...  \n",
       "3       1        peopl receiv wildfir evacu order california  \n",
       "4       1    got sent photo rubi alaska smoke wildfir school  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare_for_ml\n",
    "def filter_words(text, english_words, filter=True):\n",
    "    if filter:\n",
    "        words = text.split()\n",
    "        text = ' '.join([word for word in words if word.lower() in english_words])\n",
    "    return text\n",
    "\n",
    "def stem_text(text):\n",
    "    porter = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text\n",
    "\n",
    "def prepare_for_ml(data, english_words):\n",
    "    data.fillna('', inplace=True)\n",
    "    data['keyword'] = data['keyword'].apply(clean_phase_1)\n",
    "    data['keyword'] = data['keyword'].apply(lambda x: filter_words(x, english_words, filter)).apply(stem_text)\n",
    "    data['processed text'] = data['text'].apply(clean_phase_1)\n",
    "    data['processed text'] = data['processed text'].apply(lambda x: filter_words(x, english_words, filter)).apply(stem_text)\n",
    "    return data\n",
    "\n",
    "df = prepare_for_ml(df, english_words)\n",
    "test_set = prepare_for_ml(test_set, english_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\" style=\"margin-top: 0px\">\n",
    "<h1> Vectorising </h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aal</th>\n",
       "      <th>aba</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablaz</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abomin</th>\n",
       "      <th>...</th>\n",
       "      <th>zaman</th>\n",
       "      <th>zar</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5507 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aal  aba  abandon       abc  abil  abject  abl  ablaz  aboard  abomin  \\\n",
       "0     0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "1     0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "2     0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "3     0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "4     0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "...   ...  ...      ...       ...   ...     ...  ...    ...     ...     ...   \n",
       "7608  0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "7609  0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "7610  0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "7611  0.0  0.0      0.0  0.000000   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "7612  0.0  0.0      0.0  0.416642   0.0     0.0  0.0    0.0     0.0     0.0   \n",
       "\n",
       "      ...  zaman  zar  zeal  zionist  zip  zipper  zodiac  zombi  zone  zoom  \n",
       "0     ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "1     ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "2     ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "3     ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "4     ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "...   ...    ...  ...   ...      ...  ...     ...     ...    ...   ...   ...  \n",
       "7608  ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "7609  ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "7610  ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "7611  ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "7612  ...    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "\n",
       "[7613 rows x 5507 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df['processed text'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame (optional)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aal</th>\n",
       "      <th>aba</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablaz</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abomin</th>\n",
       "      <th>...</th>\n",
       "      <th>zaman</th>\n",
       "      <th>zar</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 5507 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aal  aba  abandon  abc  abil  abject  abl  ablaz  aboard  abomin  ...  \\\n",
       "0     0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "1     0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "2     0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "3     0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "4     0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "...   ...  ...      ...  ...   ...     ...  ...    ...     ...     ...  ...   \n",
       "3258  0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "3259  0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "3260  0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "3261  0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "3262  0.0  0.0      0.0  0.0   0.0     0.0  0.0    0.0     0.0     0.0  ...   \n",
       "\n",
       "      zaman  zar  zeal  zionist  zip  zipper  zodiac  zombi  zone  zoom  \n",
       "0       0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "1       0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "2       0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "3       0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "4       0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "...     ...  ...   ...      ...  ...     ...     ...    ...   ...   ...  \n",
       "3258    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "3259    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "3260    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "3261    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "3262    0.0  0.0   0.0      0.0  0.0     0.0     0.0    0.0   0.0   0.0  \n",
       "\n",
       "[3263 rows x 5507 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix = vectorizer.transform(test_set['processed text'].tolist())\n",
    "test_tifi = pd.DataFrame(test_matrix.toarray(), columns=vectorizer.get_feature_names_out())   \n",
    "assert(test_tifi.columns == tfidf_df.columns).all()  \n",
    "test_tifi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\" style=\"margin-top: 0px\">\n",
    "<h1> Train test split </h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a 'target' column in your DataFrame\n",
    "X = tfidf_matrix  # Use the TF-IDF matrix as features\n",
    "y = df['target']  # Assuming 'target' is the column you want to predict\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\" style=\"margin-top: 0px\">\n",
    "<h1> Model comparison</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting new model Logistic Regression LogisticRegression()\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.8024\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       874\n",
      "           1       0.82      0.69      0.75       649\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "Model saved as: Logistic Regression_model.joblib\n",
      "--------------------------------------------------\n",
      "starting new model Multinomial Naive Bayes MultinomialNB()\n",
      "\n",
      "Model: Multinomial Naive Bayes\n",
      "Accuracy: 0.8024\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84       874\n",
      "           1       0.82      0.69      0.75       649\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.81      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "Model saved as: Multinomial Naive Bayes_model.joblib\n",
      "--------------------------------------------------\n",
      "starting new model Random Forest RandomForestClassifier()\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.7827\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82       874\n",
      "           1       0.77      0.71      0.73       649\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.78      0.77      0.78      1523\n",
      "weighted avg       0.78      0.78      0.78      1523\n",
      "\n",
      "Model saved as: Random Forest_model.joblib\n",
      "--------------------------------------------------\n",
      "starting new model Support Vector Machine Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n",
      "                ('svc', SVC())])\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.7741\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.82       874\n",
      "           1       0.84      0.58      0.69       649\n",
      "\n",
      "    accuracy                           0.77      1523\n",
      "   macro avg       0.79      0.75      0.76      1523\n",
      "weighted avg       0.79      0.77      0.77      1523\n",
      "\n",
      "Model saved as: Support Vector Machine_model.joblib\n",
      "--------------------------------------------------\n",
      "starting new model CatBoost <catboost.core.CatBoostClassifier object at 0x000001AFE8F68050>\n",
      "0:\tlearn: 0.6834564\ttotal: 236ms\tremaining: 23.4s\n",
      "1:\tlearn: 0.6776171\ttotal: 275ms\tremaining: 13.5s\n",
      "2:\tlearn: 0.6691418\ttotal: 307ms\tremaining: 9.94s\n",
      "3:\tlearn: 0.6627472\ttotal: 340ms\tremaining: 8.15s\n",
      "4:\tlearn: 0.6581506\ttotal: 372ms\tremaining: 7.07s\n",
      "5:\tlearn: 0.6540129\ttotal: 404ms\tremaining: 6.33s\n",
      "6:\tlearn: 0.6483018\ttotal: 436ms\tremaining: 5.79s\n",
      "7:\tlearn: 0.6427299\ttotal: 469ms\tremaining: 5.39s\n",
      "8:\tlearn: 0.6382700\ttotal: 500ms\tremaining: 5.06s\n",
      "9:\tlearn: 0.6349146\ttotal: 532ms\tremaining: 4.79s\n",
      "10:\tlearn: 0.6317076\ttotal: 564ms\tremaining: 4.56s\n",
      "11:\tlearn: 0.6277057\ttotal: 595ms\tremaining: 4.36s\n",
      "12:\tlearn: 0.6231750\ttotal: 627ms\tremaining: 4.2s\n",
      "13:\tlearn: 0.6200744\ttotal: 660ms\tremaining: 4.05s\n",
      "14:\tlearn: 0.6163831\ttotal: 692ms\tremaining: 3.92s\n",
      "15:\tlearn: 0.6134543\ttotal: 724ms\tremaining: 3.8s\n",
      "16:\tlearn: 0.6107584\ttotal: 757ms\tremaining: 3.69s\n",
      "17:\tlearn: 0.6088956\ttotal: 789ms\tremaining: 3.6s\n",
      "18:\tlearn: 0.6056430\ttotal: 821ms\tremaining: 3.5s\n",
      "19:\tlearn: 0.6037253\ttotal: 853ms\tremaining: 3.41s\n",
      "20:\tlearn: 0.6014447\ttotal: 883ms\tremaining: 3.32s\n",
      "21:\tlearn: 0.5987071\ttotal: 909ms\tremaining: 3.22s\n",
      "22:\tlearn: 0.5965336\ttotal: 933ms\tremaining: 3.12s\n",
      "23:\tlearn: 0.5941055\ttotal: 957ms\tremaining: 3.03s\n",
      "24:\tlearn: 0.5916007\ttotal: 981ms\tremaining: 2.94s\n",
      "25:\tlearn: 0.5901249\ttotal: 1s\tremaining: 2.86s\n",
      "26:\tlearn: 0.5887038\ttotal: 1.03s\tremaining: 2.79s\n",
      "27:\tlearn: 0.5871602\ttotal: 1.07s\tremaining: 2.74s\n",
      "28:\tlearn: 0.5846575\ttotal: 1.1s\tremaining: 2.69s\n",
      "29:\tlearn: 0.5826686\ttotal: 1.13s\tremaining: 2.63s\n",
      "30:\tlearn: 0.5811671\ttotal: 1.15s\tremaining: 2.56s\n",
      "31:\tlearn: 0.5795503\ttotal: 1.18s\tremaining: 2.5s\n",
      "32:\tlearn: 0.5777930\ttotal: 1.2s\tremaining: 2.44s\n",
      "33:\tlearn: 0.5759781\ttotal: 1.23s\tremaining: 2.38s\n",
      "34:\tlearn: 0.5746296\ttotal: 1.25s\tremaining: 2.32s\n",
      "35:\tlearn: 0.5734763\ttotal: 1.27s\tremaining: 2.27s\n",
      "36:\tlearn: 0.5718407\ttotal: 1.3s\tremaining: 2.21s\n",
      "37:\tlearn: 0.5708445\ttotal: 1.32s\tremaining: 2.16s\n",
      "38:\tlearn: 0.5691899\ttotal: 1.35s\tremaining: 2.12s\n",
      "39:\tlearn: 0.5677678\ttotal: 1.38s\tremaining: 2.08s\n",
      "40:\tlearn: 0.5664065\ttotal: 1.42s\tremaining: 2.05s\n",
      "41:\tlearn: 0.5648294\ttotal: 1.46s\tremaining: 2.01s\n",
      "42:\tlearn: 0.5629686\ttotal: 1.49s\tremaining: 1.97s\n",
      "43:\tlearn: 0.5618764\ttotal: 1.52s\tremaining: 1.93s\n",
      "44:\tlearn: 0.5607351\ttotal: 1.55s\tremaining: 1.89s\n",
      "45:\tlearn: 0.5596179\ttotal: 1.58s\tremaining: 1.85s\n",
      "46:\tlearn: 0.5582253\ttotal: 1.61s\tremaining: 1.82s\n",
      "47:\tlearn: 0.5568672\ttotal: 1.64s\tremaining: 1.78s\n",
      "48:\tlearn: 0.5556639\ttotal: 1.68s\tremaining: 1.74s\n",
      "49:\tlearn: 0.5542984\ttotal: 1.71s\tremaining: 1.71s\n",
      "50:\tlearn: 0.5525659\ttotal: 1.74s\tremaining: 1.67s\n",
      "51:\tlearn: 0.5515248\ttotal: 1.77s\tremaining: 1.63s\n",
      "52:\tlearn: 0.5506354\ttotal: 1.8s\tremaining: 1.59s\n",
      "53:\tlearn: 0.5490129\ttotal: 1.83s\tremaining: 1.56s\n",
      "54:\tlearn: 0.5477655\ttotal: 1.86s\tremaining: 1.52s\n",
      "55:\tlearn: 0.5463580\ttotal: 1.89s\tremaining: 1.48s\n",
      "56:\tlearn: 0.5451467\ttotal: 1.91s\tremaining: 1.44s\n",
      "57:\tlearn: 0.5442744\ttotal: 1.94s\tremaining: 1.4s\n",
      "58:\tlearn: 0.5436272\ttotal: 1.96s\tremaining: 1.36s\n",
      "59:\tlearn: 0.5422585\ttotal: 1.99s\tremaining: 1.32s\n",
      "60:\tlearn: 0.5412310\ttotal: 2.01s\tremaining: 1.29s\n",
      "61:\tlearn: 0.5403274\ttotal: 2.04s\tremaining: 1.25s\n",
      "62:\tlearn: 0.5393743\ttotal: 2.07s\tremaining: 1.22s\n",
      "63:\tlearn: 0.5386670\ttotal: 2.1s\tremaining: 1.18s\n",
      "64:\tlearn: 0.5377902\ttotal: 2.13s\tremaining: 1.15s\n",
      "65:\tlearn: 0.5369320\ttotal: 2.15s\tremaining: 1.11s\n",
      "66:\tlearn: 0.5359075\ttotal: 2.18s\tremaining: 1.07s\n",
      "67:\tlearn: 0.5351135\ttotal: 2.2s\tremaining: 1.04s\n",
      "68:\tlearn: 0.5340099\ttotal: 2.23s\tremaining: 1s\n",
      "69:\tlearn: 0.5331873\ttotal: 2.25s\tremaining: 965ms\n",
      "70:\tlearn: 0.5323000\ttotal: 2.28s\tremaining: 930ms\n",
      "71:\tlearn: 0.5313038\ttotal: 2.3s\tremaining: 896ms\n",
      "72:\tlearn: 0.5301101\ttotal: 2.33s\tremaining: 862ms\n",
      "73:\tlearn: 0.5293774\ttotal: 2.36s\tremaining: 828ms\n",
      "74:\tlearn: 0.5284625\ttotal: 2.39s\tremaining: 797ms\n",
      "75:\tlearn: 0.5275151\ttotal: 2.42s\tremaining: 765ms\n",
      "76:\tlearn: 0.5264444\ttotal: 2.45s\tremaining: 733ms\n",
      "77:\tlearn: 0.5256588\ttotal: 2.49s\tremaining: 703ms\n",
      "78:\tlearn: 0.5244715\ttotal: 2.52s\tremaining: 671ms\n",
      "79:\tlearn: 0.5234765\ttotal: 2.55s\tremaining: 638ms\n",
      "80:\tlearn: 0.5224362\ttotal: 2.58s\tremaining: 606ms\n",
      "81:\tlearn: 0.5214002\ttotal: 2.61s\tremaining: 574ms\n",
      "82:\tlearn: 0.5206963\ttotal: 2.64s\tremaining: 542ms\n",
      "83:\tlearn: 0.5198131\ttotal: 2.68s\tremaining: 510ms\n",
      "84:\tlearn: 0.5188262\ttotal: 2.71s\tremaining: 478ms\n",
      "85:\tlearn: 0.5180737\ttotal: 2.74s\tremaining: 446ms\n",
      "86:\tlearn: 0.5169345\ttotal: 2.77s\tremaining: 414ms\n",
      "87:\tlearn: 0.5159687\ttotal: 2.8s\tremaining: 382ms\n",
      "88:\tlearn: 0.5147611\ttotal: 2.83s\tremaining: 350ms\n",
      "89:\tlearn: 0.5140674\ttotal: 2.85s\tremaining: 317ms\n",
      "90:\tlearn: 0.5131087\ttotal: 2.88s\tremaining: 285ms\n",
      "91:\tlearn: 0.5122716\ttotal: 2.9s\tremaining: 253ms\n",
      "92:\tlearn: 0.5111877\ttotal: 2.93s\tremaining: 221ms\n",
      "93:\tlearn: 0.5102052\ttotal: 2.95s\tremaining: 189ms\n",
      "94:\tlearn: 0.5089473\ttotal: 2.98s\tremaining: 157ms\n",
      "95:\tlearn: 0.5077477\ttotal: 3s\tremaining: 125ms\n",
      "96:\tlearn: 0.5065071\ttotal: 3.03s\tremaining: 93.7ms\n",
      "97:\tlearn: 0.5055289\ttotal: 3.05s\tremaining: 62.3ms\n",
      "98:\tlearn: 0.5046301\ttotal: 3.09s\tremaining: 31.2ms\n",
      "99:\tlearn: 0.5035417\ttotal: 3.12s\tremaining: 0us\n",
      "\n",
      "Model: CatBoost\n",
      "Accuracy: 0.7617\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.82       874\n",
      "           1       0.83      0.55      0.66       649\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.78      0.73      0.74      1523\n",
      "weighted avg       0.78      0.76      0.75      1523\n",
      "\n",
      "Model saved as: CatBoost_model.joblib\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': make_pipeline(StandardScaler(with_mean=False), SVC()),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, loss_function='Logloss')\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print('starting new model', name, model)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_filename = f'{name}_model.joblib'\n",
    "    joblib.dump(model, model_filename)\n",
    "    \n",
    "    # Display results\n",
    "    print(f'\\nModel: {name}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print('Classification Report:\\n', report)\n",
    "    print(f'Model saved as: {model_filename}')\n",
    "    print('--------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\" style=\"margin-top: 0px\">\n",
    "<h1> best accuracy of 80.24% </h1>\n",
    "    achieved in two models: Logistic regression and Multinomial Naive Bayes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-success\" style=\"margin-top: 0px\">\n",
    "<h1> Model evaluation </h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model predictions for re-evaluation\n",
    "chosen_model = 'Multinomial Naive Bayes_model.joblib'\n",
    "test_df = df[df.index.isin(y_test.index)]\n",
    "\n",
    "def make_predictions(saved_model_filename, new_data, text_column, vectorizer):\n",
    "    model = joblib.load(saved_model_filename)\n",
    "    new_inputs = new_data[text_column].tolist()\n",
    "    new_inputs_vectorized = vectorizer.transform(new_inputs)\n",
    "    predictions = model.predict(new_inputs_vectorized)\n",
    "\n",
    "    # Create a DataFrame with input texts and predictions\n",
    "    result_df = pd.DataFrame({'processed text': new_inputs, 'predicted': predictions})\n",
    "    return result_df\n",
    "\n",
    "predictions = make_predictions(chosen_model, test_df, 'processed text', vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>processed text</th>\n",
       "      <th>expected</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>reason earthquak may allah forgiv</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir school</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>flood disast heavi rain caus flash flood stree...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>flood florida day lost count</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>10482</td>\n",
       "      <td>@WBCShirl2 Yes God doessnt change  he says not...</td>\n",
       "      <td>ye god chang say rejoic fall peopl like wild f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7487</th>\n",
       "      <td>10709</td>\n",
       "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
       "      <td>emot train wreck bodi train wreck wreck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>10776</td>\n",
       "      <td>Wreckage 'Conclusively Confirmed' as From MH37...</td>\n",
       "      <td>wreckag conclus confirm malaysia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>10846</td>\n",
       "      <td>Heat wave warning aa? Ayyo dei. Just when I pl...</td>\n",
       "      <td>heat wave warn plan visit friend year</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>10850</td>\n",
       "      <td>NWS: Flash Flood Warning Continued for Shelby ...</td>\n",
       "      <td>nw flash flood warn continu counti wednesday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6779 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "0         1  Our Deeds are the Reason of this #earthquake M...   \n",
       "2         5  All residents asked to 'shelter in place' are ...   \n",
       "4         7  Just got sent this photo from Ruby #Alaska as ...   \n",
       "6        10  #flood #disaster Heavy rain causes flash flood...   \n",
       "12       18  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "...     ...                                                ...   \n",
       "7322  10482  @WBCShirl2 Yes God doessnt change  he says not...   \n",
       "7487  10709  My emotions are a train wreck. My body is a tr...   \n",
       "7537  10776  Wreckage 'Conclusively Confirmed' as From MH37...   \n",
       "7591  10846  Heat wave warning aa? Ayyo dei. Just when I pl...   \n",
       "7595  10850  NWS: Flash Flood Warning Continued for Shelby ...   \n",
       "\n",
       "                                         processed text  expected  predicted  \n",
       "0                     reason earthquak may allah forgiv         1          1  \n",
       "2     resid ask shelter place notifi offic evacu she...         1          1  \n",
       "4       got sent photo rubi alaska smoke wildfir school         1          0  \n",
       "6     flood disast heavi rain caus flash flood stree...         1          1  \n",
       "12                         flood florida day lost count         1          1  \n",
       "...                                                 ...       ...        ...  \n",
       "7322  ye god chang say rejoic fall peopl like wild f...         0          0  \n",
       "7487            emot train wreck bodi train wreck wreck         0          0  \n",
       "7537                   wreckag conclus confirm malaysia         1          1  \n",
       "7591              heat wave warn plan visit friend year         1          1  \n",
       "7595       nw flash flood warn continu counti wednesday         1          1  \n",
       "\n",
       "[6779 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if interested in cross-fold predictions:\n",
    "def cross_fold_predictions(saved_model_filename, X, y, df):\n",
    "    X_train_cf, X_test_cf, y_train_cf, y_test_cf = train_test_split(X, y, test_size=0.2)\n",
    "    model = joblib.load(saved_model_filename)\n",
    "    y_pred_cf = model.predict(X_test_cf)\n",
    "    pred_df = pd.DataFrame({'expected': y_test_cf, 'predicted': y_pred_cf})\n",
    "    pred_df = df[['id','text', 'processed text']].merge(pred_df, left_index=True, right_index=True)\n",
    "    return pred_df\n",
    "\n",
    "cf_predictions = []\n",
    "for trial in range(10):\n",
    "    pred_df = cross_fold_predictions(chosen_model, X, y, df)\n",
    "    cf_predictions.append(pred_df)\n",
    "\n",
    "cf_df = pd.concat(cf_predictions)\n",
    "cf_df = cf_df.drop_duplicates()\n",
    "cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english_words3.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_df.to_csv('re_evaluation_set.csv', index=False)\n",
    "joblib.dump(vectorizer, 'vectorizer3.pkl')\n",
    "joblib.dump(english_words, 'english_words3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-evaluation of the model can be found in part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
